{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karenbennis/Xy/blob/ml_model/pyspark_pipeline_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x2IxWo3wiqn",
        "colab_type": "text"
      },
      "source": [
        "<br>**Connect to Database**<br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nCS6homIgnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "b8b9ffa3-5ff0-43ce-f41f-01a33cd03b34"
      },
      "source": [
        "# Install Java, Spark, and Findspark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "#Interact with SQL\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar\n",
        "\n",
        "# Start Spark Session(Creating spark application with name defined by appName()) ---IMPORTED WITH EVERY COLAB NOTEBOOK\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"database_transformation\").config(\"spark.driver.memory\",\"8g\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-25 18:02:45--  https://jdbc.postgresql.org/download/postgresql-42.2.9.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 914037 (893K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.9.jar.2’\n",
            "\n",
            "\rpostgresql-42.2.9.j   0%[                    ]       0  --.-KB/s               \rpostgresql-42.2.9.j   4%[                    ]  40.00K   127KB/s               \rpostgresql-42.2.9.j  24%[===>                ] 216.00K   342KB/s               \rpostgresql-42.2.9.j 100%[===================>] 892.61K  1.05MB/s    in 0.8s    \n",
            "\n",
            "2020-07-25 18:02:47 (1.05 MB/s) - ‘postgresql-42.2.9.jar.2’ saved [914037/914037]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVTkr-xleT8C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb553ddf-339b-424b-c6be-d8119eb7b448"
      },
      "source": [
        "# gcloud login and check the DB\n",
        "!gcloud auth login\n",
        "!gcloud config set project 'xy-yelp'\n",
        "!gcloud sql instances describe 'xy-yelp'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&code_challenge=TGHQzzth5e5AMSwqAw6CpiH3wec9voSu-hui-MK-he0&code_challenge_method=S256&access_type=offline&response_type=code&prompt=select_account\n",
            "\n",
            "\n",
            "Enter verification code: 4/2QHe7YUZLypxZoQQ6PhcY2igPX1F0aY8znLdi8Pv6YIxOPyVrLPM_vU\n",
            "\n",
            "You are now logged in as [helenly25@gmail.com].\n",
            "Your current project is [xy-yelp].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n",
            "Updated property [core/project].\n",
            "backendType: SECOND_GEN\n",
            "connectionName: xy-yelp:northamerica-northeast1:xy-yelp\n",
            "databaseVersion: POSTGRES_12\n",
            "etag: bc66e3b4861eccb5afd42f9c257f2fb71f9c6273f5cfcb70659b019d99a17896\n",
            "gceZone: northamerica-northeast1-a\n",
            "instanceType: CLOUD_SQL_INSTANCE\n",
            "ipAddresses:\n",
            "- ipAddress: 34.95.0.17\n",
            "  type: PRIMARY\n",
            "kind: sql#instance\n",
            "name: xy-yelp\n",
            "project: xy-yelp\n",
            "region: northamerica-northeast1\n",
            "selfLink: https://sqladmin.googleapis.com/sql/v1beta4/projects/xy-yelp/instances/xy-yelp\n",
            "serverCaCert:\n",
            "  cert: |-\n",
            "    -----BEGIN CERTIFICATE-----\n",
            "    MIIDfzCCAmegAwIBAgIBADANBgkqhkiG9w0BAQsFADB3MS0wKwYDVQQuEyQzYTcz\n",
            "    ZmJhNS0xYTNlLTQwNmMtOWZhOC1hZGE1MDI3OGZkY2QxIzAhBgNVBAMTGkdvb2ds\n",
            "    ZSBDbG91ZCBTUUwgU2VydmVyIENBMRQwEgYDVQQKEwtHb29nbGUsIEluYzELMAkG\n",
            "    A1UEBhMCVVMwHhcNMjAwNzExMjAwMDA4WhcNMzAwNzA5MjAwMTA4WjB3MS0wKwYD\n",
            "    VQQuEyQzYTczZmJhNS0xYTNlLTQwNmMtOWZhOC1hZGE1MDI3OGZkY2QxIzAhBgNV\n",
            "    BAMTGkdvb2dsZSBDbG91ZCBTUUwgU2VydmVyIENBMRQwEgYDVQQKEwtHb29nbGUs\n",
            "    IEluYzELMAkGA1UEBhMCVVMwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIB\n",
            "    AQDMLAiCVUS0u8JRUQSqXLNcXiDU4euUjKhmvbOnDCr2VNVX+dQoq8P+HmL4ruxr\n",
            "    lmLgtNKuhKjqISSzVKDxEf0mudWfOP+ygsXSP6XxLWplHn/N1A881+u+T33u5N0/\n",
            "    wreqH6suvi7Cu/SSEcmYUCnpIy5pSZzO63ww9M4p2HZnG/g/j7uqGdPUpdG4GbCk\n",
            "    jb8EskJiH0hU08ct+AySyc0mrgvfP9l/S5chz0j69uQX/yvWXWy4SSfJ3+wc45Se\n",
            "    8v7WYo+lcwsOu0lGCz53X9Nej0Av47aKUzXD4HlleeMTbkjgYEv2hRWKVoUhzugT\n",
            "    dJTsminBKtnYK1n2witgxNqVAgMBAAGjFjAUMBIGA1UdEwEB/wQIMAYBAf8CAQAw\n",
            "    DQYJKoZIhvcNAQELBQADggEBAB45K3XfFNL/Vkbj0f9daDji+cJHeUS3Y8Uxehog\n",
            "    2rf+rbmwfeH757NZT29o8rSMMfV05z3fuYvS67JvSIDx/I3UsxRGzRZY4EfXXhC9\n",
            "    Vke7AVtfxNgKk86V+JPEGQp52L+PFYdh33DigL8XrWpektxIxf7RAqFIvV0F+0FS\n",
            "    DhXWXkZ8ONFpMK6BCd4aZcKgD0LHafoAKv67+A8IE6b1fFJLh1nkuqZyv1kwLwTM\n",
            "    bLzSMmkgvxuv8d0dmVxWWr/V1kjc2U7oMqF+tgOE+hcF7fWRyRRZko1RtPwjUHlT\n",
            "    qzw5uqZUsxR8S4yyiaeePiF8YnDI72ypyY6FiRMlteAydJs=\n",
            "    -----END CERTIFICATE-----\n",
            "  certSerialNumber: '0'\n",
            "  commonName: C=US,O=Google\\, Inc,CN=Google Cloud SQL Server CA,dnQualifier=3a73fba5-1a3e-406c-9fa8-ada50278fdcd\n",
            "  createTime: '2020-07-11T20:00:08.018Z'\n",
            "  expirationTime: '2030-07-09T20:01:08.018Z'\n",
            "  instance: xy-yelp\n",
            "  kind: sql#sslCert\n",
            "  sha1Fingerprint: 2e2ffdd64d766e372537c174a3d4b91f59ee3e63\n",
            "serviceAccountEmailAddress: p8056368877-0mi1n8@gcp-sa-cloud-sql.iam.gserviceaccount.com\n",
            "settings:\n",
            "  activationPolicy: ALWAYS\n",
            "  availabilityType: ZONAL\n",
            "  backupConfiguration:\n",
            "    enabled: true\n",
            "    kind: sql#backupConfiguration\n",
            "    location: us\n",
            "    pointInTimeRecoveryEnabled: true\n",
            "    replicationLogArchivingEnabled: true\n",
            "    startTime: 13:00\n",
            "  dataDiskSizeGb: '43'\n",
            "  dataDiskType: PD_SSD\n",
            "  ipConfiguration:\n",
            "    authorizedNetworks:\n",
            "    - kind: sql#aclEntry\n",
            "      name: Karen's Residence\n",
            "      value: 173.32.183.48\n",
            "    - kind: sql#aclEntry\n",
            "      name: Helen's Residence\n",
            "      value: 72.142.66.43\n",
            "    - kind: sql#aclEntry\n",
            "      name: Blake Residence\n",
            "      value: 24.114.82.234\n",
            "    ipv4Enabled: true\n",
            "  kind: sql#settings\n",
            "  locationPreference:\n",
            "    kind: sql#locationPreference\n",
            "    zone: northamerica-northeast1-a\n",
            "  maintenanceWindow:\n",
            "    day: 0\n",
            "    hour: 0\n",
            "    kind: sql#maintenanceWindow\n",
            "  pricingPlan: PER_USE\n",
            "  replicationType: SYNCHRONOUS\n",
            "  settingsVersion: '35'\n",
            "  storageAutoResize: true\n",
            "  storageAutoResizeLimit: '0'\n",
            "  tier: db-f1-micro\n",
            "state: RUNNABLE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn_F729PexE6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0fab4c63-7393-4262-9b8f-85bb74197391"
      },
      "source": [
        "# download and initialize the psql proxy\n",
        "!wget https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64 -O cloud_sql_proxy\n",
        "!chmod +x cloud_sql_proxy\n",
        "# \"connectionName\" is from the previous block\n",
        "!nohup ./cloud_sql_proxy -instances=\"xy-yelp:northamerica-northeast1:xy-yelp\"=tcp:5432 &\n",
        "!sleep 30s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cloud_sql_proxy: Text file busy\n",
            "nohup: appending output to 'nohup.out'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5Ko5J-Ee7Ec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db_password = 'kjhbyelpdb'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs2eCTjpfEd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configure settings for RDS\n",
        "mode = \"append\"\n",
        "jdbc_url=\"jdbc:postgresql://127.0.0.1:5432/xy_yelp_db\"\n",
        "config = {\"user\":\"postgres\", \n",
        "          \"password\": db_password, \n",
        "          \"driver\":\"org.postgresql.Driver\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQez-IxYJC_V",
        "colab_type": "text"
      },
      "source": [
        "**Extract tables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uudQwo30fFA-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "925eb681-60c5-40f6-ebaf-4b24e963b43a"
      },
      "source": [
        "# Pull review table\n",
        "review_df2 = spark.read.jdbc(url=jdbc_url, table='review',properties=config)\n",
        "review_df2.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----+----+------+-----+-----------+-----------+\n",
            "|           review_id|         review_text|stars|cool|useful|funny|review_date|review_type|\n",
            "+--------------------+--------------------+-----+----+------+-----+-----------+-----------+\n",
            "|cALYebKb5hygdKHql...|This is a very in...|    4|   0|     0|    0| 2011-01-12|     review|\n",
            "|SawdMXLYD5ytRmMFv...|I LOVE Chic Nails...|    5|   0|     2|    0| 2011-01-20|     review|\n",
            "|j-jMQdELr6AFkCcEH...|After the Padres ...|    5|   0|     0|    0| 2011-01-06|     review|\n",
            "|SmUMyCUNrT9HEo_DX...|I have to admit t...|    4|   0|     1|    0| 2010-01-17|     review|\n",
            "|oTB_mpCKcu-8wayQQ...|Best food, super ...|    5|   0|     1|    0| 2011-01-14|     review|\n",
            "+--------------------+--------------------+-----+----+------+-----+-----------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pmJIJiDfRb1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "5d09fbc3-2477-4f81-ccfc-ec26b3c67308"
      },
      "source": [
        "# Pull business table\n",
        "business_df2 = spark.read.jdbc(url=jdbc_url, table='business',properties=config)\n",
        "business_df2.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|           review_id|         business_id|\n",
            "+--------------------+--------------------+\n",
            "|fWKvX83p0-ka4JS3d...|9yKzy9PApeiPPOUJE...|\n",
            "|IjZ33sJrzXqU-0X6U...|ZRJwVLyzEJq1VAihD...|\n",
            "|IESLBzqUCLdSzSqm0...|6oRAC4uyJCsJl1X0W...|\n",
            "|G-WvGaISbqqaMHlNn...|_1QQZuf4zZOyFCvXc...|\n",
            "|1uJFq2r5QfJG_6ExM...|6ozycU1RpktNG2-1B...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFb2iKvOfRh6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "e346f8a3-87b4-4a9d-d632-9b0829b0bdf7"
      },
      "source": [
        "# Pull yelp_user table\n",
        "user_df2 = spark.read.jdbc(url=jdbc_url, table='yelp_user',properties=config)\n",
        "user_df2.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|           review_id|             user_id|\n",
            "+--------------------+--------------------+\n",
            "|GJGUHAAONtBSBj53c...|Z3c7xGRfeV-uMkSbA...|\n",
            "|nQH2KAvAeOJOYKX99...|ryjqXdp68i2I9JPOp...|\n",
            "|-yKcbjWSlmKC1zTMT...|5W-ruHmpkwLyI6Lla...|\n",
            "|20aES_-g5Vyqfzojn...|vhxFLqRok6r-D_aQz...|\n",
            "|W_d9w7yr3koSUXHco...|aBnKTxZzdhabTXfzt...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTo3oLx-fRoX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "4563c439-ccec-4d8f-8fe5-66db32f293e9"
      },
      "source": [
        "# Join tables\n",
        "spark_df = review_df2.join(business_df2, on=\"review_id\", how=\"inner\")\n",
        "spark_df = spark_df.join(user_df2, on=\"review_id\", how=\"inner\")\n",
        "spark_df.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----+----+------+-----+-----------+-----------+--------------------+--------------------+\n",
            "|           review_id|         review_text|stars|cool|useful|funny|review_date|review_type|         business_id|             user_id|\n",
            "+--------------------+--------------------+-----+----+------+-----+-----------+-----------+--------------------+--------------------+\n",
            "|-7yxrdY13ay15rGB7...|I have been going...|    5|   0|     0|    0| 2010-01-16|     review|Lh9nz0KYyzE-YRbKu...|ayKW9eWwGFcrtJaHc...|\n",
            "|-Be0UUGYuiDJVAM_Y...|Since Im big into...|    4|   0|     2|    2| 2011-01-25|     review|pa6K7DGByxBXxcVJ5...|_4lqpCYCqOQzbB6xQ...|\n",
            "|-nQHHXi-d_yuW301_...|A pleasant place ...|    2|   0|     0|    0| 2011-01-12|     review|GIGI8bJfN6HyPzmEW...|4QORbyhfN01oKR_Gg...|\n",
            "|2L30O7G8IQ6HILpR0...|part of a social ...|    5|   0|     0|    0| 2010-01-24|     review|qiwajZigq_2twTmYo...|ST8Yzlk2MqKlcaLqL...|\n",
            "|4x5yLG7_yGLuN-w6f...|I love every plac...|    4|   0|     1|    0| 2011-01-02|     review|9yKzy9PApeiPPOUJE...|Vk-hJ1i5ZagPM87Kv...|\n",
            "+--------------------+--------------------+-----+----+------+-----+-----------+-----------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6k3WVfJM6cE",
        "colab_type": "text"
      },
      "source": [
        "**Transformation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6_h62s7vUEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark_df=spark_df.withColumn('length',F.length('review_text'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5UOtjZNA0BR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "9b439d37-fe7f-4b7e-ea8f-4cd28d910c2b"
      },
      "source": [
        "spark_df = spark_df.filter((spark_df.stars != 2) & (spark_df.stars != 4))\n",
        "spark_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----+----+------+-----+-----------+-----------+--------------------+--------------------+------+\n",
            "|           review_id|         review_text|stars|cool|useful|funny|review_date|review_type|         business_id|             user_id|length|\n",
            "+--------------------+--------------------+-----+----+------+-----+-----------+-----------+--------------------+--------------------+------+\n",
            "|-7yxrdY13ay15rGB7...|I have been going...|    5|   0|     0|    0| 2010-01-16|     review|Lh9nz0KYyzE-YRbKu...|ayKW9eWwGFcrtJaHc...|   670|\n",
            "|2L30O7G8IQ6HILpR0...|part of a social ...|    5|   0|     0|    0| 2010-01-24|     review|qiwajZigq_2twTmYo...|ST8Yzlk2MqKlcaLqL...|   415|\n",
            "|5h0EVAee-RDbbKfhd...|A great value for...|    5|   2|     1|    0| 2012-01-21|     review|4VzaYvZntWBRbr8Vm...|feQpvbp8jGBWMuG5u...|   284|\n",
            "|6jV77Bs_Vu_rHkdUx...|This review is fo...|    3|   1|     2|    1| 2011-01-23|     review|kwq3bK7BzPKLwXKqV...|RwVaQNP1Ag-Seu3U9...|  1092|\n",
            "|8uV26l7-ktb4tZ5gR...|The new Harkins C...|    5|   0|     1|    0| 2007-01-23|     review|1pGC-0jvQ6vN5rzmC...|nWouNfZD3Pw08RYiz...|   355|\n",
            "|AfDVtSpM6GvkqS1wJ...|This is one of my...|    5|   0|     0|    0| 2010-01-07|     review|nvaAUTTl7oqiJDhui...|LvAr6xG-A40PGk1qu...|   185|\n",
            "|CdAkiPi_KKLjHo5l2...|OK, I went based ...|    5|   1|     5|    1| 2008-01-11|     review|NB6o73F05lw23zpC7...|AdEy5KAIlMAy8xHyu...|  1712|\n",
            "|EnAdKZ_u_wj9ifTRw...|I highly reccomen...|    5|   0|     1|    0| 2012-01-12|     review|rncjoVoEFUJGCUoC1...|HK35ai8frY75iMYBV...|   308|\n",
            "|FBw_MLyO9eduoDfs-...|I was severely mi...|    1|   3|     5|   20| 2011-01-22|     review|MPsCdY0Bwv2G0JXtw...|lhf22sqBafQv6CXu2...|   203|\n",
            "|HPJHQKDVttoNjsnlC...|Where else can yo...|    5|   1|     1|    0| 2012-01-29|     review|5KG0A3WlC7K3DAXtr...|ZRQdOIIhzrzE6sc3e...|   460|\n",
            "|MEdGp-pkf6DAxAsgU...|Awesome food and ...|    5|   0|     0|    0| 2012-01-15|     review|I4bSn5gXsHuSPu7L-...|JfezQ-BqHOE7pnW2D...|    25|\n",
            "|PINoAXjJQpl3yobdD...|Today I went in f...|    3|   0|     0|    0| 2011-01-16|     review|v6zRA0WqOODJjmpvs...|Iycf9KNRhxvR187Qu...|   342|\n",
            "|Q5tzwCvyTZBn6_5N1...|The novelty of th...|    3|   4|     4|    4| 2009-01-24|     review|Zg-C1aYcoR2L5OIrA...|DPMUGzrjTwX5uJtrm...|  1157|\n",
            "|VO1mJpGWPKRdGnJuw...|It does not get m...|    5|   3|     3|    3| 2008-01-15|     review|bcBMAa0UQpNLFvvdZ...|YavFbDG7DUOTXJBLx...|   664|\n",
            "|WNfddsdJW2u3Cd9Mq...|May I add \"family...|    5|   0|     0|    0| 2012-01-23|     review|UScgPn6pIHuOmQJTs...|7W4egS-WyR0hRHMfQ...|   628|\n",
            "|X7Z6rtSPvKOw2TvQx...|Eh, this place is...|    3|   2|     3|    0| 2007-01-26|     review|FUI-hWH_bpis7AKZT...|e4TQFVfepzHf--hnB...|   558|\n",
            "|ZQNhHraFM-ErSZdgh...|Fantastic.  Absol...|    5|   0|     0|    1| 2012-01-05|     review|wNsmt1hF1uv3YvbwX...|1CL12c9zuE36NGhx-...|   704|\n",
            "|aw3lJKaOQy7ayEQ3z...|Just like the oth...|    3|   0|     0|    0| 2012-01-16|     review|irVjrnurmB03bTaj9...|IjafRfMSAQpInLlw-...|   846|\n",
            "|h8wpsGAv2jMNLVk_g...|I am totally addi...|    5|   0|     0|    0| 2008-01-12|     review|Oz1w_3Ck8lalmtxPc...|toD5fpe7--5ljkAz1...|   181|\n",
            "|kX8e5NsQu7fHf221k...|Indiana Universit...|    5|  13|     4|    6| 2008-01-01|     review|hvg4NfkaV-aVqbMzK...|mOirLg2h76o-dPn97...|  1926|\n",
            "+--------------------+--------------------+-----+----+------+-----+-----------+-----------+--------------------+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIcXTo1KKDDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# spark_df=spark_df.withColumn('class',F.when( (spark_df[\"stars\"]==1), 0).when((spark_df[\"stars\"]))\n",
        "# spark_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYCeG56KqO75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "c7de1ac5-aec2-40c7-f806-ccfec870b24f"
      },
      "source": [
        "# convert to pandas\n",
        "import pandas as pd\n",
        "pandas_df = pd.read_csv(\"https://raw.githubusercontent.com/karenbennis/Xy/storyboard/uniform_yelp.csv\")\n",
        "\n",
        "# Set index\n",
        "# pandas_df = pandas_df.set_index('review_id')\n",
        "\n",
        "pandas_df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>useful</th>\n",
              "      <th>funny</th>\n",
              "      <th>cool</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>K8avYPWsh45v7VoZgQz-Vg</td>\n",
              "      <td>mrEoKZsYtSgMJsIuLr_oTg</td>\n",
              "      <td>UGyEr_PMA-v1cuim0gMPlQ</td>\n",
              "      <td>3</td>\n",
              "      <td>2008-11-23</td>\n",
              "      <td>another pie place that I would be happy to mee...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BkiZn5XSzAv9q7J7_4HP7A</td>\n",
              "      <td>7HFkF9A2xaHxLqwcFnJ0JQ</td>\n",
              "      <td>N_2yEZ41g9zDW_gWArFiHw</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-03-30</td>\n",
              "      <td>I came with my sister and was very excited fro...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>L6kc7Nr7hWiqo7ZvWu4rtg</td>\n",
              "      <td>rsi9Sf_Efk55hPRk6k7vpQ</td>\n",
              "      <td>XhLM_OtYslzyd4GyvpqACA</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-03-19</td>\n",
              "      <td>I am very disappointed with the Brazilian I re...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>y35xKzutHXT985mUpp5qVA</td>\n",
              "      <td>fKSX1CAoXkyQecAWR02DVQ</td>\n",
              "      <td>ILa-Xv5-h23A9OMrYK17oQ</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-05-04</td>\n",
              "      <td>Stopped for lunch with my wife. I had the Mont...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>UqQGtBDEfkYMLV-FykNCdA</td>\n",
              "      <td>YmTYfzrC4Z2JbyOucGVJtA</td>\n",
              "      <td>gBfPyzPRmeOaj3SdcIj0Rw</td>\n",
              "      <td>1</td>\n",
              "      <td>2010-05-20</td>\n",
              "      <td>DON'T DO IT!  You're in Vegas, looking for a d...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                review_id                 user_id  ... funny  cool\n",
              "0  K8avYPWsh45v7VoZgQz-Vg  mrEoKZsYtSgMJsIuLr_oTg  ...     0     0\n",
              "1  BkiZn5XSzAv9q7J7_4HP7A  7HFkF9A2xaHxLqwcFnJ0JQ  ...     0     1\n",
              "2  L6kc7Nr7hWiqo7ZvWu4rtg  rsi9Sf_Efk55hPRk6k7vpQ  ...     0     0\n",
              "3  y35xKzutHXT985mUpp5qVA  fKSX1CAoXkyQecAWR02DVQ  ...     0     1\n",
              "4  UqQGtBDEfkYMLV-FykNCdA  YmTYfzrC4Z2JbyOucGVJtA  ...     0     0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-oZ7ztJqPAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dependencies for nltk\n",
        "# https://towardsdatascience.com/natural-language-processing-nlp-for-machine-learning-d44498845d5b\n",
        "import nltk"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWjqAnpGsxtM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8fca16d4-7693-48fc-a999-4f0aa1f9999a"
      },
      "source": [
        "# Import string and punctuations\n",
        "import string\n",
        "string.punctuation"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUy5O7uUsx3o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "b8b2f5db-b863-4184-b031-6231a9672b62"
      },
      "source": [
        "# Function to remove Punctuation\n",
        "def remove_punct(text):\n",
        "\n",
        "  # Discard all punctuations\n",
        "  text_nopunct = ''.join([char for char in text if char not in string.punctuation])\n",
        "  return text_nopunct\n",
        "\n",
        "pandas_df['body_text_clean'] = pandas_df['text'].apply(lambda x: remove_punct(x))\n",
        "\n",
        "pandas_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>useful</th>\n",
              "      <th>funny</th>\n",
              "      <th>cool</th>\n",
              "      <th>body_text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>K8avYPWsh45v7VoZgQz-Vg</td>\n",
              "      <td>mrEoKZsYtSgMJsIuLr_oTg</td>\n",
              "      <td>UGyEr_PMA-v1cuim0gMPlQ</td>\n",
              "      <td>3</td>\n",
              "      <td>2008-11-23</td>\n",
              "      <td>another pie place that I would be happy to mee...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>another pie place that I would be happy to mee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BkiZn5XSzAv9q7J7_4HP7A</td>\n",
              "      <td>7HFkF9A2xaHxLqwcFnJ0JQ</td>\n",
              "      <td>N_2yEZ41g9zDW_gWArFiHw</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-03-30</td>\n",
              "      <td>I came with my sister and was very excited fro...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>I came with my sister and was very excited fro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>L6kc7Nr7hWiqo7ZvWu4rtg</td>\n",
              "      <td>rsi9Sf_Efk55hPRk6k7vpQ</td>\n",
              "      <td>XhLM_OtYslzyd4GyvpqACA</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-03-19</td>\n",
              "      <td>I am very disappointed with the Brazilian I re...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I am very disappointed with the Brazilian I re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>y35xKzutHXT985mUpp5qVA</td>\n",
              "      <td>fKSX1CAoXkyQecAWR02DVQ</td>\n",
              "      <td>ILa-Xv5-h23A9OMrYK17oQ</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-05-04</td>\n",
              "      <td>Stopped for lunch with my wife. I had the Mont...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Stopped for lunch with my wife I had the Monte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>UqQGtBDEfkYMLV-FykNCdA</td>\n",
              "      <td>YmTYfzrC4Z2JbyOucGVJtA</td>\n",
              "      <td>gBfPyzPRmeOaj3SdcIj0Rw</td>\n",
              "      <td>1</td>\n",
              "      <td>2010-05-20</td>\n",
              "      <td>DON'T DO IT!  You're in Vegas, looking for a d...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DONT DO IT  Youre in Vegas looking for a dinin...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                review_id  ...                                    body_text_clean\n",
              "0  K8avYPWsh45v7VoZgQz-Vg  ...  another pie place that I would be happy to mee...\n",
              "1  BkiZn5XSzAv9q7J7_4HP7A  ...  I came with my sister and was very excited fro...\n",
              "2  L6kc7Nr7hWiqo7ZvWu4rtg  ...  I am very disappointed with the Brazilian I re...\n",
              "3  y35xKzutHXT985mUpp5qVA  ...  Stopped for lunch with my wife I had the Monte...\n",
              "4  UqQGtBDEfkYMLV-FykNCdA  ...  DONT DO IT  Youre in Vegas looking for a dinin...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x1pQ29xsx-L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "ee963629-5779-45a1-bfff-25badd1964b5"
      },
      "source": [
        "# Tokenization\n",
        "import re\n",
        "\n",
        "# Function to Tokenize words\n",
        "def tokenize(text):\n",
        "\n",
        "  # W+ means that either a word character (A-Za-z0-9) or a dash (-) can go there\n",
        "  tokens = re.split('\\W+', text)\n",
        "  return tokens\n",
        "\n",
        "# Convert to lowercase as Python is case-sensitive\n",
        "pandas_df['body_text_tokenized'] = pandas_df['body_text_clean'].apply(lambda x: tokenize(x.lower()))\n",
        "\n",
        "pandas_df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>useful</th>\n",
              "      <th>funny</th>\n",
              "      <th>cool</th>\n",
              "      <th>body_text_clean</th>\n",
              "      <th>body_text_tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>K8avYPWsh45v7VoZgQz-Vg</td>\n",
              "      <td>mrEoKZsYtSgMJsIuLr_oTg</td>\n",
              "      <td>UGyEr_PMA-v1cuim0gMPlQ</td>\n",
              "      <td>3</td>\n",
              "      <td>2008-11-23</td>\n",
              "      <td>another pie place that I would be happy to mee...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>another pie place that I would be happy to mee...</td>\n",
              "      <td>[another, pie, place, that, i, would, be, happ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BkiZn5XSzAv9q7J7_4HP7A</td>\n",
              "      <td>7HFkF9A2xaHxLqwcFnJ0JQ</td>\n",
              "      <td>N_2yEZ41g9zDW_gWArFiHw</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-03-30</td>\n",
              "      <td>I came with my sister and was very excited fro...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>I came with my sister and was very excited fro...</td>\n",
              "      <td>[i, came, with, my, sister, and, was, very, ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>L6kc7Nr7hWiqo7ZvWu4rtg</td>\n",
              "      <td>rsi9Sf_Efk55hPRk6k7vpQ</td>\n",
              "      <td>XhLM_OtYslzyd4GyvpqACA</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-03-19</td>\n",
              "      <td>I am very disappointed with the Brazilian I re...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I am very disappointed with the Brazilian I re...</td>\n",
              "      <td>[i, am, very, disappointed, with, the, brazili...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>y35xKzutHXT985mUpp5qVA</td>\n",
              "      <td>fKSX1CAoXkyQecAWR02DVQ</td>\n",
              "      <td>ILa-Xv5-h23A9OMrYK17oQ</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-05-04</td>\n",
              "      <td>Stopped for lunch with my wife. I had the Mont...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Stopped for lunch with my wife I had the Monte...</td>\n",
              "      <td>[stopped, for, lunch, with, my, wife, i, had, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>UqQGtBDEfkYMLV-FykNCdA</td>\n",
              "      <td>YmTYfzrC4Z2JbyOucGVJtA</td>\n",
              "      <td>gBfPyzPRmeOaj3SdcIj0Rw</td>\n",
              "      <td>1</td>\n",
              "      <td>2010-05-20</td>\n",
              "      <td>DON'T DO IT!  You're in Vegas, looking for a d...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DONT DO IT  Youre in Vegas looking for a dinin...</td>\n",
              "      <td>[dont, do, it, youre, in, vegas, looking, for,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                review_id  ...                                body_text_tokenized\n",
              "0  K8avYPWsh45v7VoZgQz-Vg  ...  [another, pie, place, that, i, would, be, happ...\n",
              "1  BkiZn5XSzAv9q7J7_4HP7A  ...  [i, came, with, my, sister, and, was, very, ex...\n",
              "2  L6kc7Nr7hWiqo7ZvWu4rtg  ...  [i, am, very, disappointed, with, the, brazili...\n",
              "3  y35xKzutHXT985mUpp5qVA  ...  [stopped, for, lunch, with, my, wife, i, had, ...\n",
              "4  UqQGtBDEfkYMLV-FykNCdA  ...  [dont, do, it, youre, in, vegas, looking, for,...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER0d1Pb3ulKl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6717ea9f-5ebb-4d4d-9c53-0784747ae5f7"
      },
      "source": [
        "# Remove all English stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stopword = nltk.corpus.stopwords.words('english') "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA4exlyhulOs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "f038577d-570e-4d5f-9e4b-8ddfbafaeea2"
      },
      "source": [
        "# Function to remove stopwords\n",
        "def remove_stopwords(tokenized_list):\n",
        "\n",
        "  # Remove all stopwords\n",
        "  text = [word for word in tokenized_list if word not in stopword]\n",
        "  return text\n",
        "\n",
        "pandas_df['body_text_nostop'] = pandas_df['body_text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
        "\n",
        "pandas_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>useful</th>\n",
              "      <th>funny</th>\n",
              "      <th>cool</th>\n",
              "      <th>body_text_clean</th>\n",
              "      <th>body_text_tokenized</th>\n",
              "      <th>body_text_nostop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>K8avYPWsh45v7VoZgQz-Vg</td>\n",
              "      <td>mrEoKZsYtSgMJsIuLr_oTg</td>\n",
              "      <td>UGyEr_PMA-v1cuim0gMPlQ</td>\n",
              "      <td>3</td>\n",
              "      <td>2008-11-23</td>\n",
              "      <td>another pie place that I would be happy to mee...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>another pie place that I would be happy to mee...</td>\n",
              "      <td>[another, pie, place, that, i, would, be, happ...</td>\n",
              "      <td>[another, pie, place, would, happy, meet, good...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BkiZn5XSzAv9q7J7_4HP7A</td>\n",
              "      <td>7HFkF9A2xaHxLqwcFnJ0JQ</td>\n",
              "      <td>N_2yEZ41g9zDW_gWArFiHw</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-03-30</td>\n",
              "      <td>I came with my sister and was very excited fro...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>I came with my sister and was very excited fro...</td>\n",
              "      <td>[i, came, with, my, sister, and, was, very, ex...</td>\n",
              "      <td>[came, sister, excited, hype, social, media, g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>L6kc7Nr7hWiqo7ZvWu4rtg</td>\n",
              "      <td>rsi9Sf_Efk55hPRk6k7vpQ</td>\n",
              "      <td>XhLM_OtYslzyd4GyvpqACA</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-03-19</td>\n",
              "      <td>I am very disappointed with the Brazilian I re...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I am very disappointed with the Brazilian I re...</td>\n",
              "      <td>[i, am, very, disappointed, with, the, brazili...</td>\n",
              "      <td>[disappointed, brazilian, received, today, got...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>y35xKzutHXT985mUpp5qVA</td>\n",
              "      <td>fKSX1CAoXkyQecAWR02DVQ</td>\n",
              "      <td>ILa-Xv5-h23A9OMrYK17oQ</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-05-04</td>\n",
              "      <td>Stopped for lunch with my wife. I had the Mont...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Stopped for lunch with my wife I had the Monte...</td>\n",
              "      <td>[stopped, for, lunch, with, my, wife, i, had, ...</td>\n",
              "      <td>[stopped, lunch, wife, monte, cristo, potato, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>UqQGtBDEfkYMLV-FykNCdA</td>\n",
              "      <td>YmTYfzrC4Z2JbyOucGVJtA</td>\n",
              "      <td>gBfPyzPRmeOaj3SdcIj0Rw</td>\n",
              "      <td>1</td>\n",
              "      <td>2010-05-20</td>\n",
              "      <td>DON'T DO IT!  You're in Vegas, looking for a d...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DONT DO IT  Youre in Vegas looking for a dinin...</td>\n",
              "      <td>[dont, do, it, youre, in, vegas, looking, for,...</td>\n",
              "      <td>[dont, youre, vegas, looking, dining, hot, spo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                review_id  ...                                   body_text_nostop\n",
              "0  K8avYPWsh45v7VoZgQz-Vg  ...  [another, pie, place, would, happy, meet, good...\n",
              "1  BkiZn5XSzAv9q7J7_4HP7A  ...  [came, sister, excited, hype, social, media, g...\n",
              "2  L6kc7Nr7hWiqo7ZvWu4rtg  ...  [disappointed, brazilian, received, today, got...\n",
              "3  y35xKzutHXT985mUpp5qVA  ...  [stopped, lunch, wife, monte, cristo, potato, ...\n",
              "4  UqQGtBDEfkYMLV-FykNCdA  ...  [dont, youre, vegas, looking, dining, hot, spo...\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XOi9l2zulSP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "76692ebd-9ea0-4a6c-de9f-0d3191558140"
      },
      "source": [
        "# Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Create an instance for stemmer\n",
        "ps = nltk.PorterStemmer()\n",
        "\n",
        "# Function for stemming\n",
        "def stemming(tokenized_text):\n",
        "\n",
        "  text = [ps.stem(word) for word in tokenized_text]\n",
        "  return text\n",
        "\n",
        "pandas_df['body_text_stemmed'] = pandas_df['body_text_nostop'].apply(lambda x: stemming(x))\n",
        "\n",
        "pandas_df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>useful</th>\n",
              "      <th>funny</th>\n",
              "      <th>cool</th>\n",
              "      <th>body_text_clean</th>\n",
              "      <th>body_text_tokenized</th>\n",
              "      <th>body_text_nostop</th>\n",
              "      <th>body_text_stemmed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>K8avYPWsh45v7VoZgQz-Vg</td>\n",
              "      <td>mrEoKZsYtSgMJsIuLr_oTg</td>\n",
              "      <td>UGyEr_PMA-v1cuim0gMPlQ</td>\n",
              "      <td>3</td>\n",
              "      <td>2008-11-23</td>\n",
              "      <td>another pie place that I would be happy to mee...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>another pie place that I would be happy to mee...</td>\n",
              "      <td>[another, pie, place, that, i, would, be, happ...</td>\n",
              "      <td>[another, pie, place, would, happy, meet, good...</td>\n",
              "      <td>[anoth, pie, place, would, happi, meet, good, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BkiZn5XSzAv9q7J7_4HP7A</td>\n",
              "      <td>7HFkF9A2xaHxLqwcFnJ0JQ</td>\n",
              "      <td>N_2yEZ41g9zDW_gWArFiHw</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-03-30</td>\n",
              "      <td>I came with my sister and was very excited fro...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>I came with my sister and was very excited fro...</td>\n",
              "      <td>[i, came, with, my, sister, and, was, very, ex...</td>\n",
              "      <td>[came, sister, excited, hype, social, media, g...</td>\n",
              "      <td>[came, sister, excit, hype, social, media, got...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>L6kc7Nr7hWiqo7ZvWu4rtg</td>\n",
              "      <td>rsi9Sf_Efk55hPRk6k7vpQ</td>\n",
              "      <td>XhLM_OtYslzyd4GyvpqACA</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-03-19</td>\n",
              "      <td>I am very disappointed with the Brazilian I re...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I am very disappointed with the Brazilian I re...</td>\n",
              "      <td>[i, am, very, disappointed, with, the, brazili...</td>\n",
              "      <td>[disappointed, brazilian, received, today, got...</td>\n",
              "      <td>[disappoint, brazilian, receiv, today, got, ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>y35xKzutHXT985mUpp5qVA</td>\n",
              "      <td>fKSX1CAoXkyQecAWR02DVQ</td>\n",
              "      <td>ILa-Xv5-h23A9OMrYK17oQ</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-05-04</td>\n",
              "      <td>Stopped for lunch with my wife. I had the Mont...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Stopped for lunch with my wife I had the Monte...</td>\n",
              "      <td>[stopped, for, lunch, with, my, wife, i, had, ...</td>\n",
              "      <td>[stopped, lunch, wife, monte, cristo, potato, ...</td>\n",
              "      <td>[stop, lunch, wife, mont, cristo, potato, sala...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>UqQGtBDEfkYMLV-FykNCdA</td>\n",
              "      <td>YmTYfzrC4Z2JbyOucGVJtA</td>\n",
              "      <td>gBfPyzPRmeOaj3SdcIj0Rw</td>\n",
              "      <td>1</td>\n",
              "      <td>2010-05-20</td>\n",
              "      <td>DON'T DO IT!  You're in Vegas, looking for a d...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DONT DO IT  Youre in Vegas looking for a dinin...</td>\n",
              "      <td>[dont, do, it, youre, in, vegas, looking, for,...</td>\n",
              "      <td>[dont, youre, vegas, looking, dining, hot, spo...</td>\n",
              "      <td>[dont, your, vega, look, dine, hot, spot, your...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                review_id  ...                                  body_text_stemmed\n",
              "0  K8avYPWsh45v7VoZgQz-Vg  ...  [anoth, pie, place, would, happi, meet, good, ...\n",
              "1  BkiZn5XSzAv9q7J7_4HP7A  ...  [came, sister, excit, hype, social, media, got...\n",
              "2  L6kc7Nr7hWiqo7ZvWu4rtg  ...  [disappoint, brazilian, receiv, today, got, ho...\n",
              "3  y35xKzutHXT985mUpp5qVA  ...  [stop, lunch, wife, mont, cristo, potato, sala...\n",
              "4  UqQGtBDEfkYMLV-FykNCdA  ...  [dont, your, vega, look, dine, hot, spot, your...\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NhasPSiuldn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "outputId": "0d4c162a-3f75-4be4-fdf1-67aeb8dafbae"
      },
      "source": [
        "# Lemmatization\n",
        "# import these modules \n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "# Create an instance\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "\n",
        "def lemmatizing(tokenized_text):\n",
        "\n",
        "  text = [wn.lemmatize(word) for word in tokenized_text]\n",
        "  return text\n",
        "\n",
        "pandas_df['body_text_lemmatized'] = pandas_df['body_text_nostop'].apply(lambda x: lemmatizing(x))\n",
        "\n",
        "pandas_df.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>useful</th>\n",
              "      <th>funny</th>\n",
              "      <th>cool</th>\n",
              "      <th>body_text_clean</th>\n",
              "      <th>body_text_tokenized</th>\n",
              "      <th>body_text_nostop</th>\n",
              "      <th>body_text_stemmed</th>\n",
              "      <th>body_text_lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>K8avYPWsh45v7VoZgQz-Vg</td>\n",
              "      <td>mrEoKZsYtSgMJsIuLr_oTg</td>\n",
              "      <td>UGyEr_PMA-v1cuim0gMPlQ</td>\n",
              "      <td>3</td>\n",
              "      <td>2008-11-23</td>\n",
              "      <td>another pie place that I would be happy to mee...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>another pie place that I would be happy to mee...</td>\n",
              "      <td>[another, pie, place, that, i, would, be, happ...</td>\n",
              "      <td>[another, pie, place, would, happy, meet, good...</td>\n",
              "      <td>[anoth, pie, place, would, happi, meet, good, ...</td>\n",
              "      <td>[another, pie, place, would, happy, meet, good...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BkiZn5XSzAv9q7J7_4HP7A</td>\n",
              "      <td>7HFkF9A2xaHxLqwcFnJ0JQ</td>\n",
              "      <td>N_2yEZ41g9zDW_gWArFiHw</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-03-30</td>\n",
              "      <td>I came with my sister and was very excited fro...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>I came with my sister and was very excited fro...</td>\n",
              "      <td>[i, came, with, my, sister, and, was, very, ex...</td>\n",
              "      <td>[came, sister, excited, hype, social, media, g...</td>\n",
              "      <td>[came, sister, excit, hype, social, media, got...</td>\n",
              "      <td>[came, sister, excited, hype, social, medium, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>L6kc7Nr7hWiqo7ZvWu4rtg</td>\n",
              "      <td>rsi9Sf_Efk55hPRk6k7vpQ</td>\n",
              "      <td>XhLM_OtYslzyd4GyvpqACA</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-03-19</td>\n",
              "      <td>I am very disappointed with the Brazilian I re...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I am very disappointed with the Brazilian I re...</td>\n",
              "      <td>[i, am, very, disappointed, with, the, brazili...</td>\n",
              "      <td>[disappointed, brazilian, received, today, got...</td>\n",
              "      <td>[disappoint, brazilian, receiv, today, got, ho...</td>\n",
              "      <td>[disappointed, brazilian, received, today, got...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>y35xKzutHXT985mUpp5qVA</td>\n",
              "      <td>fKSX1CAoXkyQecAWR02DVQ</td>\n",
              "      <td>ILa-Xv5-h23A9OMrYK17oQ</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-05-04</td>\n",
              "      <td>Stopped for lunch with my wife. I had the Mont...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Stopped for lunch with my wife I had the Monte...</td>\n",
              "      <td>[stopped, for, lunch, with, my, wife, i, had, ...</td>\n",
              "      <td>[stopped, lunch, wife, monte, cristo, potato, ...</td>\n",
              "      <td>[stop, lunch, wife, mont, cristo, potato, sala...</td>\n",
              "      <td>[stopped, lunch, wife, monte, cristo, potato, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>UqQGtBDEfkYMLV-FykNCdA</td>\n",
              "      <td>YmTYfzrC4Z2JbyOucGVJtA</td>\n",
              "      <td>gBfPyzPRmeOaj3SdcIj0Rw</td>\n",
              "      <td>1</td>\n",
              "      <td>2010-05-20</td>\n",
              "      <td>DON'T DO IT!  You're in Vegas, looking for a d...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DONT DO IT  Youre in Vegas looking for a dinin...</td>\n",
              "      <td>[dont, do, it, youre, in, vegas, looking, for,...</td>\n",
              "      <td>[dont, youre, vegas, looking, dining, hot, spo...</td>\n",
              "      <td>[dont, your, vega, look, dine, hot, spot, your...</td>\n",
              "      <td>[dont, youre, vega, looking, dining, hot, spot...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                review_id  ...                               body_text_lemmatized\n",
              "0  K8avYPWsh45v7VoZgQz-Vg  ...  [another, pie, place, would, happy, meet, good...\n",
              "1  BkiZn5XSzAv9q7J7_4HP7A  ...  [came, sister, excited, hype, social, medium, ...\n",
              "2  L6kc7Nr7hWiqo7ZvWu4rtg  ...  [disappointed, brazilian, received, today, got...\n",
              "3  y35xKzutHXT985mUpp5qVA  ...  [stopped, lunch, wife, monte, cristo, potato, ...\n",
              "4  UqQGtBDEfkYMLV-FykNCdA  ...  [dont, youre, vega, looking, dining, hot, spot...\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMmolEh9Frk8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "471cca66-614e-44cb-9da0-c3679f6ad0c8"
      },
      "source": [
        "pandas_df['length'] = pandas_df['text'].apply(len)\n",
        "pandas_df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>business_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>useful</th>\n",
              "      <th>funny</th>\n",
              "      <th>cool</th>\n",
              "      <th>body_text_clean</th>\n",
              "      <th>body_text_tokenized</th>\n",
              "      <th>body_text_nostop</th>\n",
              "      <th>body_text_stemmed</th>\n",
              "      <th>body_text_lemmatized</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>K8avYPWsh45v7VoZgQz-Vg</td>\n",
              "      <td>mrEoKZsYtSgMJsIuLr_oTg</td>\n",
              "      <td>UGyEr_PMA-v1cuim0gMPlQ</td>\n",
              "      <td>3</td>\n",
              "      <td>2008-11-23</td>\n",
              "      <td>another pie place that I would be happy to mee...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>another pie place that I would be happy to mee...</td>\n",
              "      <td>[another, pie, place, that, i, would, be, happ...</td>\n",
              "      <td>[another, pie, place, would, happy, meet, good...</td>\n",
              "      <td>[anoth, pie, place, would, happi, meet, good, ...</td>\n",
              "      <td>[another, pie, place, would, happy, meet, good...</td>\n",
              "      <td>256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BkiZn5XSzAv9q7J7_4HP7A</td>\n",
              "      <td>7HFkF9A2xaHxLqwcFnJ0JQ</td>\n",
              "      <td>N_2yEZ41g9zDW_gWArFiHw</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-03-30</td>\n",
              "      <td>I came with my sister and was very excited fro...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>I came with my sister and was very excited fro...</td>\n",
              "      <td>[i, came, with, my, sister, and, was, very, ex...</td>\n",
              "      <td>[came, sister, excited, hype, social, media, g...</td>\n",
              "      <td>[came, sister, excit, hype, social, media, got...</td>\n",
              "      <td>[came, sister, excited, hype, social, medium, ...</td>\n",
              "      <td>480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>L6kc7Nr7hWiqo7ZvWu4rtg</td>\n",
              "      <td>rsi9Sf_Efk55hPRk6k7vpQ</td>\n",
              "      <td>XhLM_OtYslzyd4GyvpqACA</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-03-19</td>\n",
              "      <td>I am very disappointed with the Brazilian I re...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>I am very disappointed with the Brazilian I re...</td>\n",
              "      <td>[i, am, very, disappointed, with, the, brazili...</td>\n",
              "      <td>[disappointed, brazilian, received, today, got...</td>\n",
              "      <td>[disappoint, brazilian, receiv, today, got, ho...</td>\n",
              "      <td>[disappointed, brazilian, received, today, got...</td>\n",
              "      <td>236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>y35xKzutHXT985mUpp5qVA</td>\n",
              "      <td>fKSX1CAoXkyQecAWR02DVQ</td>\n",
              "      <td>ILa-Xv5-h23A9OMrYK17oQ</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-05-04</td>\n",
              "      <td>Stopped for lunch with my wife. I had the Mont...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Stopped for lunch with my wife I had the Monte...</td>\n",
              "      <td>[stopped, for, lunch, with, my, wife, i, had, ...</td>\n",
              "      <td>[stopped, lunch, wife, monte, cristo, potato, ...</td>\n",
              "      <td>[stop, lunch, wife, mont, cristo, potato, sala...</td>\n",
              "      <td>[stopped, lunch, wife, monte, cristo, potato, ...</td>\n",
              "      <td>265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>UqQGtBDEfkYMLV-FykNCdA</td>\n",
              "      <td>YmTYfzrC4Z2JbyOucGVJtA</td>\n",
              "      <td>gBfPyzPRmeOaj3SdcIj0Rw</td>\n",
              "      <td>1</td>\n",
              "      <td>2010-05-20</td>\n",
              "      <td>DON'T DO IT!  You're in Vegas, looking for a d...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>DONT DO IT  Youre in Vegas looking for a dinin...</td>\n",
              "      <td>[dont, do, it, youre, in, vegas, looking, for,...</td>\n",
              "      <td>[dont, youre, vegas, looking, dining, hot, spo...</td>\n",
              "      <td>[dont, your, vega, look, dine, hot, spot, your...</td>\n",
              "      <td>[dont, youre, vega, looking, dining, hot, spot...</td>\n",
              "      <td>3123</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                review_id  ... length\n",
              "0  K8avYPWsh45v7VoZgQz-Vg  ...    256\n",
              "1  BkiZn5XSzAv9q7J7_4HP7A  ...    480\n",
              "2  L6kc7Nr7hWiqo7ZvWu4rtg  ...    236\n",
              "3  y35xKzutHXT985mUpp5qVA  ...    265\n",
              "4  UqQGtBDEfkYMLV-FykNCdA  ...   3123\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-5BWzzTultW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "outputId": "3a26714d-baab-435d-e022-8097b28dfd1a"
      },
      "source": [
        "# Apply CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "X_counts = count_vect.fit_transform(pandas_df['review_text'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'review_text'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-13cce4f19533>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcount_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandas_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'review_text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_F1-5saxzIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# X_counts_df = pd.DataFrame(X_counts.toarray(), columns=count_vect.get_feature_names())\n",
        "# X_counts_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vbt3mBbxzMW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "ed6a46a1-9780-48d6-fcb7-9c9957e09299"
      },
      "source": [
        "pandas_df_copy = pandas_df.copy()\n",
        "\n",
        "pandas_df_copy = pandas_df_copy[['review_id', 'text', 'stars', 'cool', 'useful', 'funny', 'date', 'business_id', 'user_id', 'length', 'body_text_nostop', 'body_text_stemmed', 'body_text_lemmatized']]\n",
        "\n",
        "# Convert pandas_df to sparks df\n",
        "spark_df = spark.createDataFrame(pandas_df_copy)\n",
        "spark_df.show(5)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----+----+------+-----+----------+--------------------+--------------------+------+--------------------+--------------------+--------------------+\n",
            "|           review_id|                text|stars|cool|useful|funny|      date|         business_id|             user_id|length|    body_text_nostop|   body_text_stemmed|body_text_lemmatized|\n",
            "+--------------------+--------------------+-----+----+------+-----+----------+--------------------+--------------------+------+--------------------+--------------------+--------------------+\n",
            "|K8avYPWsh45v7VoZg...|another pie place...|    3|   0|     0|    0|2008-11-23|UGyEr_PMA-v1cuim0...|mrEoKZsYtSgMJsIuL...|   256|[another, pie, pl...|[anoth, pie, plac...|[another, pie, pl...|\n",
            "|BkiZn5XSzAv9q7J7_...|I came with my si...|    4|   1|     1|    0|2017-03-30|N_2yEZ41g9zDW_gWA...|7HFkF9A2xaHxLqwcF...|   480|[came, sister, ex...|[came, sister, ex...|[came, sister, ex...|\n",
            "|L6kc7Nr7hWiqo7ZvW...|I am very disappo...|    1|   0|     0|    0|2017-03-19|XhLM_OtYslzyd4Gyv...|rsi9Sf_Efk55hPRk6...|   236|[disappointed, br...|[disappoint, braz...|[disappointed, br...|\n",
            "|y35xKzutHXT985mUp...|Stopped for lunch...|    4|   1|     0|    0|2017-05-04|ILa-Xv5-h23A9OMrY...|fKSX1CAoXkyQecAWR...|   265|[stopped, lunch, ...|[stop, lunch, wif...|[stopped, lunch, ...|\n",
            "|UqQGtBDEfkYMLV-Fy...|DON'T DO IT!  You...|    1|   0|     1|    0|2010-05-20|gBfPyzPRmeOaj3Sdc...|YmTYfzrC4Z2JbyOuc...|  3123|[dont, youre, veg...|[dont, your, vega...|[dont, youre, veg...|\n",
            "+--------------------+--------------------+-----+----+------+-----+----------+--------------------+--------------------+------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kiylt7xOxzVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import functions\n",
        "from pyspark.ml.feature import HashingTF, IDF, StringIndexer"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr4QTe4p4Qdm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "87107140-c986-4688-d9d8-0d09a09ed214"
      },
      "source": [
        "# Make stars values a list\n",
        "from pyspark.sql.functions import col, split\n",
        "spark_df = spark_df.withColumn(\"star_array\", split(col(\"stars\"), \" \"))\n",
        "spark_df.show()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----+----+------+-----+----------+--------------------+--------------------+------+--------------------+--------------------+--------------------+----------+\n",
            "|           review_id|                text|stars|cool|useful|funny|      date|         business_id|             user_id|length|    body_text_nostop|   body_text_stemmed|body_text_lemmatized|star_array|\n",
            "+--------------------+--------------------+-----+----+------+-----+----------+--------------------+--------------------+------+--------------------+--------------------+--------------------+----------+\n",
            "|K8avYPWsh45v7VoZg...|another pie place...|    3|   0|     0|    0|2008-11-23|UGyEr_PMA-v1cuim0...|mrEoKZsYtSgMJsIuL...|   256|[another, pie, pl...|[anoth, pie, plac...|[another, pie, pl...|       [3]|\n",
            "|BkiZn5XSzAv9q7J7_...|I came with my si...|    4|   1|     1|    0|2017-03-30|N_2yEZ41g9zDW_gWA...|7HFkF9A2xaHxLqwcF...|   480|[came, sister, ex...|[came, sister, ex...|[came, sister, ex...|       [4]|\n",
            "|L6kc7Nr7hWiqo7ZvW...|I am very disappo...|    1|   0|     0|    0|2017-03-19|XhLM_OtYslzyd4Gyv...|rsi9Sf_Efk55hPRk6...|   236|[disappointed, br...|[disappoint, braz...|[disappointed, br...|       [1]|\n",
            "|y35xKzutHXT985mUp...|Stopped for lunch...|    4|   1|     0|    0|2017-05-04|ILa-Xv5-h23A9OMrY...|fKSX1CAoXkyQecAWR...|   265|[stopped, lunch, ...|[stop, lunch, wif...|[stopped, lunch, ...|       [4]|\n",
            "|UqQGtBDEfkYMLV-Fy...|DON'T DO IT!  You...|    1|   0|     1|    0|2010-05-20|gBfPyzPRmeOaj3Sdc...|YmTYfzrC4Z2JbyOuc...|  3123|[dont, youre, veg...|[dont, your, vega...|[dont, youre, veg...|       [1]|\n",
            "|vOFqQ2ixWDBsg5y3r...|I have used the s...|    5|   0|     0|    0|2017-12-07|Y854K6eDqo-II-I3g...|SAdmlwTOxA9p-1GLa...|   818|[used, services, ...|[use, servic, dav...|[used, service, d...|       [5]|\n",
            "|PTiXzzAibAMp8etKx...|Just as advertise...|    5|   0|     0|    0|2013-04-05|n8RVrpWpcd3HprzEn...|orO4O_7InPaJk32SJ...|   154|[advertised, exac...|[advertis, exactl...|[advertised, exac...|       [5]|\n",
            "|BizH_uZC69D-39xJj...|Please note that ...|    2|   0|     1|    0|2017-07-06|2iTsRqUsPGRH1li1W...|RSKAOwXvhCnJN6eOe...|   454|[please, note, di...|[pleas, note, did...|[please, note, di...|       [2]|\n",
            "|OwvSYizydT8WBK-lX...|In a nutshell, th...|    1|   0|     0|    0|2017-12-10|CSXhYcVHTN5Tvr1Sg...|pn6pWPHcKFV4OZbCE...|   489|[nutshell, guys, ...|[nutshel, guy, su...|[nutshell, guy, s...|       [1]|\n",
            "|YrcZ6K3Jlqsf4UylT...|I was in Vegas wi...|    2|   0|     3|    0|2011-08-05|ohz8ljVxcZMo2bMGr...|f7hNYpD9EK5ArGpQm...|  2013|[vegas, family, g...|[vega, famili, gr...|[vega, family, gr...|       [2]|\n",
            "|UXoaClvNTrDAlHsmk...|Just ordered and ...|    5|   0|     0|    0|2015-08-28|-NfdQz7Pesvthek59...|4t2BNnyRnwUXRcluY...|   474|[ordered, receive...|[order, receiv, d...|[ordered, receive...|       [5]|\n",
            "|eWLuz3n8HIoOCUU0t...|Got notified we (...|    2|   0|     0|    0|2017-04-21|e41TP5cXZqSrz50xC...|UcdkAE1WLEvNaVWW4...|   459|[got, notified, 3...|[got, notifi, 3, ...|[got, notified, 3...|       [2]|\n",
            "|uFIXgibZFM0ESSoXP...|Went here for the...|    4|   0|     0|    0|2016-07-13|1b7Ma0CBJ0oDPQrFO...|n6pQegZPVp6MqJNUH...|   342|[went, teppanyaki...|[went, teppanyaki...|[went, teppanyaki...|       [4]|\n",
            "|zCqNaRZHyswUvWlfm...|Lesvos has specia...|    3|   0|     0|    0|2011-10-09|mEOVr6mU6RSGqYtLz...|4IlFpxJ-HU1iBKo89...|   463|[lesvos, specials...|[lesvo, special, ...|[lesvos, special,...|       [3]|\n",
            "|nDodJqefAA1Y_k47V...|So...we got here ...|    2|   0|     2|    0|2014-11-08|3sUq_KnZs58_7LHbV...|zrO1IWna2hqDlJsTF...|   543|[sowe, got, seate...|[sow, got, seat, ...|[sowe, got, seate...|       [2]|\n",
            "|KhYIy6TO407kSwRct...|Was really pumped...|    2|   2|     8|    1|2012-03-03|qAg8GkN9rforFWmGx...|F_SLKO6rcypQbbZEm...|  1972|[really, pumped, ...|[realli, pump, fi...|[really, pumped, ...|       [2]|\n",
            "|ylOS0GOy4D1GWsnw_...|Went to her new d...|    1|   3|     8|    2|2014-11-09|-Rd76RLlOWbz2c7dj...|Ks4YILcmJ8nrnGn49...|   438|[went, new, dog, ...|[went, new, dog, ...|[went, new, dog, ...|       [1]|\n",
            "|LYMAE0lRRo_X1CscE...|The service was v...|    5|   0|     0|    0|2015-12-25|AbPQf-X7awuPFDULi...|fZjVjOlNuIR3XCb9G...|   387|[service, fast, o...|[servic, fast, or...|[service, fast, o...|       [5]|\n",
            "|CJuiMuy5a3cMek-Xk...|We had the worst ...|    1|   0|     0|    0|2016-03-18|E67gGK8yOn9kkNzx8...|WP64JyH6vk_OlHUqH...|   513|[worst, service, ...|[worst, servic, e...|[worst, service, ...|       [1]|\n",
            "|FkDB7oJRv7Kf3mQjM...|I don't understan...|    2|   0|     0|    0|2015-07-25|Kz4WS00PcqOJgxfZa...|Mi4hhmdO9Uy9qq6yW...|   348|[dont, understand...|[dont, understand...|[dont, understand...|       [2]|\n",
            "+--------------------+--------------------+-----+----+------+-----+----------+--------------------+--------------------+------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58Do7Vby4QkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize a CoutVectorizer\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "star_vectorizer = CountVectorizer(inputCol=\"star_array\", outputCol=\"stars_one_hot\", vocabSize=5, minDF=1.0)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WTHAZPK4QrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a vector model\n",
        "star_vector_model = star_vectorizer.fit(spark_df)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWwyaeRU4Q0-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "a9e38aa8-72b1-4c5f-b388-db5a411c1806"
      },
      "source": [
        "# One hot encoded column\n",
        "df_ohe = star_vector_model.transform(spark_df)\n",
        "df_ohe.show(3)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----+----+------+-----+----------+--------------------+--------------------+------+--------------------+--------------------+--------------------+----------+-------------+\n",
            "|           review_id|                text|stars|cool|useful|funny|      date|         business_id|             user_id|length|    body_text_nostop|   body_text_stemmed|body_text_lemmatized|star_array|stars_one_hot|\n",
            "+--------------------+--------------------+-----+----+------+-----+----------+--------------------+--------------------+------+--------------------+--------------------+--------------------+----------+-------------+\n",
            "|K8avYPWsh45v7VoZg...|another pie place...|    3|   0|     0|    0|2008-11-23|UGyEr_PMA-v1cuim0...|mrEoKZsYtSgMJsIuL...|   256|[another, pie, pl...|[anoth, pie, plac...|[another, pie, pl...|       [3]|(5,[2],[1.0])|\n",
            "|BkiZn5XSzAv9q7J7_...|I came with my si...|    4|   1|     1|    0|2017-03-30|N_2yEZ41g9zDW_gWA...|7HFkF9A2xaHxLqwcF...|   480|[came, sister, ex...|[came, sister, ex...|[came, sister, ex...|       [4]|(5,[0],[1.0])|\n",
            "|L6kc7Nr7hWiqo7ZvW...|I am very disappo...|    1|   0|     0|    0|2017-03-19|XhLM_OtYslzyd4Gyv...|rsi9Sf_Efk55hPRk6...|   236|[disappointed, br...|[disappoint, braz...|[disappointed, br...|       [1]|(5,[3],[1.0])|\n",
            "+--------------------+--------------------+-----+----+------+-----+----------+--------------------+--------------------+------+--------------------+--------------------+--------------------+----------+-------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNFHqcTA4Q9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create all the features to the data set\n",
        "star_rating = StringIndexer(inputCol='stars',outputCol='label')\n",
        "hashingTF = HashingTF(inputCol=\"body_text_stemmed\", outputCol='hash_token')\n",
        "idf = IDF(inputCol='hash_token', outputCol='idf_token')"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO9K26k74sf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vector\n",
        "# Create feature vector \n",
        "clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7myETso4sk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create and run a data processing Pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "data_prep_pipeline = Pipeline(stages=[star_rating, hashingTF, idf, clean_up])"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2iX37s_4spP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit and transform the pipeline\n",
        "cleaner = data_prep_pipeline.fit(df_ohe)\n",
        "cleaned = cleaner.transform(df_ohe)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhsYLkZe4RDV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "74c161d2-ec1d-4c00-ab98-6dc88d74dd00"
      },
      "source": [
        "cleaned.show(5)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+-----+----+------+-----+----------+--------------------+--------------------+------+--------------------+--------------------+--------------------+----------+-------------+-----+--------------------+--------------------+--------------------+\n",
            "|           review_id|                text|stars|cool|useful|funny|      date|         business_id|             user_id|length|    body_text_nostop|   body_text_stemmed|body_text_lemmatized|star_array|stars_one_hot|label|          hash_token|           idf_token|            features|\n",
            "+--------------------+--------------------+-----+----+------+-----+----------+--------------------+--------------------+------+--------------------+--------------------+--------------------+----------+-------------+-----+--------------------+--------------------+--------------------+\n",
            "|K8avYPWsh45v7VoZg...|another pie place...|    3|   0|     0|    0|2008-11-23|UGyEr_PMA-v1cuim0...|mrEoKZsYtSgMJsIuL...|   256|[another, pie, pl...|[anoth, pie, plac...|[another, pie, pl...|       [3]|(5,[2],[1.0])|  2.0|(262144,[22567,27...|(262144,[22567,27...|(262145,[22567,27...|\n",
            "|BkiZn5XSzAv9q7J7_...|I came with my si...|    4|   1|     1|    0|2017-03-30|N_2yEZ41g9zDW_gWA...|7HFkF9A2xaHxLqwcF...|   480|[came, sister, ex...|[came, sister, ex...|[came, sister, ex...|       [4]|(5,[0],[1.0])|  0.0|(262144,[427,1253...|(262144,[427,1253...|(262145,[427,1253...|\n",
            "|L6kc7Nr7hWiqo7ZvW...|I am very disappo...|    1|   0|     0|    0|2017-03-19|XhLM_OtYslzyd4Gyv...|rsi9Sf_Efk55hPRk6...|   236|[disappointed, br...|[disappoint, braz...|[disappointed, br...|       [1]|(5,[3],[1.0])|  3.0|(262144,[17893,28...|(262144,[17893,28...|(262145,[17893,28...|\n",
            "|y35xKzutHXT985mUp...|Stopped for lunch...|    4|   1|     0|    0|2017-05-04|ILa-Xv5-h23A9OMrY...|fKSX1CAoXkyQecAWR...|   265|[stopped, lunch, ...|[stop, lunch, wif...|[stopped, lunch, ...|       [4]|(5,[0],[1.0])|  0.0|(262144,[31463,65...|(262144,[31463,65...|(262145,[31463,65...|\n",
            "|UqQGtBDEfkYMLV-Fy...|DON'T DO IT!  You...|    1|   0|     1|    0|2010-05-20|gBfPyzPRmeOaj3Sdc...|YmTYfzrC4Z2JbyOuc...|  3123|[dont, youre, veg...|[dont, your, vega...|[dont, youre, veg...|       [1]|(5,[3],[1.0])|  3.0|(262144,[2396,392...|(262144,[2396,392...|(262145,[2396,392...|\n",
            "+--------------------+--------------------+-----+----+------+-----+----------+--------------------+--------------------+------+--------------------+--------------------+--------------------+----------+-------------+-----+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edBuycdkn10s",
        "colab_type": "text"
      },
      "source": [
        "<br></br>**Pipeline**<br></br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x0XmXV9VBTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import functions\n",
        "# from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsIhoHGLh5OK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make stars values a list\n",
        "# from pyspark.sql.functions import col, split\n",
        "# spark_df = spark_df.withColumn(\"star_array\", split(col(\"stars\"), \" \"))\n",
        "# spark_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdpCEiGLf1Nu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize a CoutVectorizer\n",
        "# from pyspark.ml.feature import CountVectorizer\n",
        "# star_vectorizer = CountVectorizer(inputCol=\"star_array\", outputCol=\"stars_one_hot\", vocabSize=5, minDF=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHSxkv0LhLSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a vector model\n",
        "# star_vector_model = star_vectorizer.fit(spark_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PIyUjlWiwg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # One hot encoded column\n",
        "# df_ohe = star_vector_model.transform(spark_df)\n",
        "# df_ohe.show(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VorHUGE_vta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create all the features to the data set\n",
        "#star_rating = StringIndexer(inputCol='stars_one_hot',outputCol='label')\n",
        "# tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"token_text\")\n",
        "# stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
        "# hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol='hash_token')\n",
        "# idf = IDF(inputCol='hash_token', outputCol='idf_token')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-05EYS6Bold9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from pyspark.ml.feature import VectorAssembler\n",
        "# from pyspark.ml.linalg import Vector\n",
        "# # Create feature vector \n",
        "# clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrqh24KqlJDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create and run a data processing Pipeline\n",
        "# from pyspark.ml import Pipeline\n",
        "# data_prep_pipeline = Pipeline(stages=[tokenizer, stopremove, hashingTF, idf, clean_up])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ_67nv5lKOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit and transform the pipeline\n",
        "# cleaner = data_prep_pipeline.fit(df_ohe)\n",
        "# cleaned = cleaner.transform(df_ohe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg-HKC5rqA4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaned.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f_yjR0QRVuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop redundant column\n",
        "# x=cleaned.drop('review_type')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuHwRtRZKII6",
        "colab_type": "text"
      },
      "source": [
        "# **Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAhQ4n_XJokp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "02902fc0-c4ee-4dc0-9037-859ffca40a88"
      },
      "source": [
        "#Drop intermediate columns\n",
        "x=cleaned.select('features', 'label')\n",
        "x.show(5)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|(262145,[22567,27...|  2.0|\n",
            "|(262145,[427,1253...|  0.0|\n",
            "|(262145,[17893,28...|  3.0|\n",
            "|(262145,[31463,65...|  0.0|\n",
            "|(262145,[2396,392...|  3.0|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOw9lb8SL2ck",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "66021545-06c7-4cc5-9c7e-945262cd1ddd"
      },
      "source": [
        "x.dtypes"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('features', 'vector'), ('label', 'double')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpfYVKpKMKwo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "654b482a-45da-4342-c69b-19681ec63a03"
      },
      "source": [
        "# Import col\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Change column DataType for stars\n",
        "x = x.withColumn('label', col('label').cast('int'))\n",
        "\n",
        "x.show(5)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|(262145,[22567,27...|    2|\n",
            "|(262145,[427,1253...|    0|\n",
            "|(262145,[17893,28...|    3|\n",
            "|(262145,[31463,65...|    0|\n",
            "|(262145,[2396,392...|    3|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBUVduBPLCad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rename class to label\n",
        "# x = x.withColumnRenamed('stars_one_hot', 'label')\n",
        "# x.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGwNrKeJVMWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import pandas as pd\n",
        "# data_df = x.select('*').toPandas()\n",
        "# x = spark.createDataFrame(data_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw4iHzHGJosY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Break data down into a training set and a testing set\n",
        "training, testing = x.randomSplit([0.7, 0.3], 21)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgrfQolZ60kp",
        "colab_type": "text"
      },
      "source": [
        "**Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQg1u3srJoyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "# Create a Naive Bayes model and fit training data\n",
        "nb = NaiveBayes()\n",
        "predictor = nb.fit(training)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1sw6FiKJovu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "0360c12d-06ce-466e-b45a-9f3d5b90bcd0"
      },
      "source": [
        "# Use the Class Evaluator for a cleaner description\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "acc_eval = MulticlassClassificationEvaluator()\n",
        "acc = acc_eval.evaluate(predictor.transform(testing))\n",
        "print(\"Accuracy of model at predicting reviews was: %f\" % acc)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of model at predicting reviews was: 0.358936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97471hZfGRe5",
        "colab_type": "text"
      },
      "source": [
        "Accuracy of model at predicting reviews was: 0.522249"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh7jpZOW6vQo",
        "colab_type": "text"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ph4zTLsJoqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "# Create a Naive Bayes model and fit training data\n",
        "lg = LogisticRegression()\n",
        "predictor = lg.fit(training)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC6ucu5KJooj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ed786d4d-3410-42a7-a962-fea25b3a941a"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "acc_eval = MulticlassClassificationEvaluator()\n",
        "acc = acc_eval.evaluate(predictor.transform(testing))\n",
        "print(\"Accuracy of model at predicting reviews was: %f\" % acc)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of model at predicting reviews was: 0.444839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5epWOPPGbHy",
        "colab_type": "text"
      },
      "source": [
        "Accuracy of model at predicting reviews was: 0.701011"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f33IsJHRH39v",
        "colab_type": "text"
      },
      "source": [
        "**Multilayer Percepron**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXTWK85rH2xY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sib4qnGSH8Zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into train and test\n",
        "splits = x.randomSplit([0.8, 0.2], 1234)\n",
        "train = splits[0]\n",
        "test = splits[1]"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "495u_f-LIc3G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "67b0483d-1f30-4b48-b390-0a91409fb851"
      },
      "source": [
        "train.show()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|(262145,[15,427,2...|  3.0|\n",
            "|(262145,[15,991,2...|  0.0|\n",
            "|(262145,[15,1353,...|  2.0|\n",
            "|(262145,[15,4914,...|  1.0|\n",
            "|(262145,[15,5381,...|  1.0|\n",
            "|(262145,[29,1353,...|  0.0|\n",
            "|(262145,[47,1353,...|  2.0|\n",
            "|(262145,[47,2325,...|  1.0|\n",
            "|(262145,[47,12325...|  1.0|\n",
            "|(262145,[47,34343...|  0.0|\n",
            "|(262145,[68,5173,...|  0.0|\n",
            "|(262145,[70,3856,...|  2.0|\n",
            "|(262145,[74,3067,...|  2.0|\n",
            "|(262145,[91,991,4...|  0.0|\n",
            "|(262145,[97,440,9...|  4.0|\n",
            "|(262145,[97,635,9...|  3.0|\n",
            "|(262145,[97,976,3...|  2.0|\n",
            "|(262145,[97,1353,...|  4.0|\n",
            "|(262145,[97,1353,...|  4.0|\n",
            "|(262145,[97,1353,...|  4.0|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzva90r5INFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specify layers for the neural network:\n",
        "# input layer of size 3 (features), two intermediate of size 5 and 4\n",
        "# and output of size 5 (classes)\n",
        "layers = [262145, 256, 5]"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvuLEMTaINRM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "outputId": "fbd194b3-b02d-43dd-8b88-f4d8aeae8e8b"
      },
      "source": [
        "# create the trainer and set its parameters\n",
        "trainer = MultilayerPerceptronClassifier(maxIter=10, layers=layers, blockSize=128, seed=1234)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
            "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
            "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
            "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
            "py4j.protocol.Py4JNetworkError: Error while receiving\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "Py4JError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-048b133cc3b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create the trainer and set its parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultilayerPerceptronClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/classification.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, featuresCol, labelCol, predictionCol, maxIter, tol, seed, layers, blockSize, stepSize, solver, initialWeights, probabilityCol, rawPredictionCol)\u001b[0m\n\u001b[1;32m   1596\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultilayerPerceptronClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         self._java_obj = self._new_java_obj(\n\u001b[0;32m-> 1598\u001b[0;31m             \"org.apache.spark.ml.classification.MultilayerPerceptronClassifier\", self.uid)\n\u001b[0m\u001b[1;32m   1599\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1E-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"l-bfgs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mjava_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjava_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1657\u001b[0m             message = compute_exception_message(\n\u001b[1;32m   1658\u001b[0m                 \"{0} does not exist in the JVM\".format(name), error_message)\n\u001b[0;32m-> 1659\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JError\u001b[0m: org does not exist in the JVM"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6qc2Z9iJRUD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f539de4c-d7d6-432c-cb63-3c95d04889f2"
      },
      "source": [
        "# train the model\n",
        "model = trainer.fit(train)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:45873)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:45873)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-72-4f6cd2c6c383>\", line 2, in <module>\n",
            "    model = trainer.fit(train)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/base.py\", line 132, in fit\n",
            "    return self._fit(dataset)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 295, in _fit\n",
            "    java_model = self._fit_java(dataset)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 291, in _fit_java\n",
            "    self._transfer_params_to_java()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 124, in _transfer_params_to_java\n",
            "    pair = self._make_java_param_pair(param, self._paramMap[param])\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 113, in _make_java_param_pair\n",
            "    java_param = self._java_obj.getParam(param.name)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1255, in __call__\n",
            "    answer = self.gateway_client.send_command(command)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 983, in send_command\n",
            "    connection = self._get_connection()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 931, in _get_connection\n",
            "    connection = self._create_connection()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 937, in _create_connection\n",
            "    connection.start()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1079, in start\n",
            "    raise Py4JNetworkError(msg, e)\n",
            "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:45873)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:45873)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-72-4f6cd2c6c383>\", line 2, in <module>\n",
            "    model = trainer.fit(train)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/base.py\", line 132, in fit\n",
            "    return self._fit(dataset)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 295, in _fit\n",
            "    java_model = self._fit_java(dataset)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 291, in _fit_java\n",
            "    self._transfer_params_to_java()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 124, in _transfer_params_to_java\n",
            "    pair = self._make_java_param_pair(param, self._paramMap[param])\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 113, in _make_java_param_pair\n",
            "    java_param = self._java_obj.getParam(param.name)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1255, in __call__\n",
            "    answer = self.gateway_client.send_command(command)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 983, in send_command\n",
            "    connection = self._get_connection()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 931, in _get_connection\n",
            "    connection = self._create_connection()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 937, in _create_connection\n",
            "    connection.start()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1079, in start\n",
            "    raise Py4JNetworkError(msg, e)\n",
            "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:45873)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:45873)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-72-4f6cd2c6c383>\", line 2, in <module>\n",
            "    model = trainer.fit(train)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/base.py\", line 132, in fit\n",
            "    return self._fit(dataset)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 295, in _fit\n",
            "    java_model = self._fit_java(dataset)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 291, in _fit_java\n",
            "    self._transfer_params_to_java()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 124, in _transfer_params_to_java\n",
            "    pair = self._make_java_param_pair(param, self._paramMap[param])\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 113, in _make_java_param_pair\n",
            "    java_param = self._java_obj.getParam(param.name)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1255, in __call__\n",
            "    answer = self.gateway_client.send_command(command)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 983, in send_command\n",
            "    connection = self._get_connection()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 931, in _get_connection\n",
            "    connection = self._create_connection()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 937, in _create_connection\n",
            "    connection.start()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1079, in start\n",
            "    raise Py4JNetworkError(msg, e)\n",
            "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:45873)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:45873)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-72-4f6cd2c6c383>\", line 2, in <module>\n",
            "    model = trainer.fit(train)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/base.py\", line 132, in fit\n",
            "    return self._fit(dataset)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 295, in _fit\n",
            "    java_model = self._fit_java(dataset)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 291, in _fit_java\n",
            "    self._transfer_params_to_java()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 124, in _transfer_params_to_java\n",
            "    pair = self._make_java_param_pair(param, self._paramMap[param])\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 113, in _make_java_param_pair\n",
            "    java_param = self._java_obj.getParam(param.name)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1255, in __call__\n",
            "    answer = self.gateway_client.send_command(command)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 983, in send_command\n",
            "    connection = self._get_connection()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 931, in _get_connection\n",
            "    connection = self._create_connection()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 937, in _create_connection\n",
            "    connection.start()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1079, in start\n",
            "    raise Py4JNetworkError(msg, e)\n",
            "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:45873)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:45873)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-72-4f6cd2c6c383>\", line 2, in <module>\n",
            "    model = trainer.fit(train)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/base.py\", line 132, in fit\n",
            "    return self._fit(dataset)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 295, in _fit\n",
            "    java_model = self._fit_java(dataset)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 291, in _fit_java\n",
            "    self._transfer_params_to_java()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 124, in _transfer_params_to_java\n",
            "    pair = self._make_java_param_pair(param, self._paramMap[param])\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\", line 113, in _make_java_param_pair\n",
            "    java_param = self._java_obj.getParam(param.name)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1255, in __call__\n",
            "    answer = self.gateway_client.send_command(command)\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 983, in send_command\n",
            "    connection = self._get_connection()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 931, in _get_connection\n",
            "    connection = self._create_connection()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 937, in _create_connection\n",
            "    connection.start()\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1079, in start\n",
            "    raise Py4JNetworkError(msg, e)\n",
            "py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:45873)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'Py4JNetworkError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
            "    connection = self.deque.pop()\n",
            "IndexError: pop from an empty deque\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
            "    self.socket.connect((self.address, self.port))\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "Py4JNetworkError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-4f6cd2c6c383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0mJava\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transfer_params_to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0mpair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_java_param_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_paramMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_make_java_param_pair\u001b[0;34m(self, param, value)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolveParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mjava_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mjava_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    981\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \"\"\"\n\u001b[0;32m--> 983\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    936\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:45873)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiYwbJB-JT0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBB8_97-JUDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8QrfnccAW8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Authenticate user\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbXgRphaAnNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set project id\n",
        "project_id = 'xy-yelp'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gZLSWomAd5m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf45a75c-d78e-4f1e-fa15-91112c8beab8"
      },
      "source": [
        "# Set project\n",
        "!gcloud config set project {project_id}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7k0dj-vBVN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# File path to save json file to\n",
        "filepath = '/tmp/ml_j.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSlyIBEoAjcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save json file **** this will break if the file already exists, which it does at this point, therefore its commented out for now\n",
        "x.coalesce(1).write.format('json').save(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ytv4w7eEEs_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bucket name\n",
        "bucket_name = 'xy-bucket'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb1-ZDoaDrGa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3635e7d8-ebd1-4304-aeab-f926dac44d35"
      },
      "source": [
        "# Copy saved file from /tmp to bucket\n",
        "!gsutil cp -r /tmp/ml_j.json/ gs://{bucket_name}/json_files/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file:///tmp/ml_j.json/.part-00000-f8a8f21c-0ebe-434c-a8b3-a5c8988dd298-c000.json.crc [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/ml_j.json/._SUCCESS.crc [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/ml_j.json/_SUCCESS [Content-Type=application/octet-stream]...\n",
            "Copying file:///tmp/ml_j.json/part-00000-f8a8f21c-0ebe-434c-a8b3-a5c8988dd298-c000.json [Content-Type=application/json]...\n",
            "\\\n",
            "Operation completed over 4 objects/63.0 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}